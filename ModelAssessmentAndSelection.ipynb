{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ModelAssessmentAndSelection.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"N5v5SPP-hcA8","executionInfo":{"status":"ok","timestamp":1601999955705,"user_tz":240,"elapsed":1222,"user":{"displayName":"Rebecca Gartenberg","photoUrl":"","userId":"14914130225385476872"}}},"source":["# ECE 475 Assignment 3: Model Assessment and Selection\n","# Re-implement the example in section 7.10.2 using any simple, out of the box classifier (like K nearest neighbors from sci-kit). \n","# Reproduce the results for the incorrect and correct way of doing cross-validation.\n","\n","# Sophie, Rebecca, and Marc\n","\n","import numpy as np\n","import pandas as pd\n","import random\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.feature_selection import SelectKBest\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1-nl6n7XwaJC","executionInfo":{"status":"ok","timestamp":1602002279551,"user_tz":240,"elapsed":967,"user":{"displayName":"Rebecca Gartenberg","photoUrl":"","userId":"14914130225385476872"}}},"source":["# Create Features\n","# Standard Gaussian Predictors 5000 predictors x 50 samples\n","mu, sigma = 0, 0.1 # mean and standard deviation\n","N = 50 # number of samples\n","p = 5000 # number of predictors \n","\n","s = np.random.normal(mu, sigma, [N,p])\n","# Create Labels\n","z = np.zeros(25)\n","o = np.ones(25)\n","labels = np.append(z, o)\n","random.shuffle(labels)\n","\n","def split_data(s):\n","\n","  percent_correct_incorrect = np.zeros(50)\n","  percent_correct_correct = np.zeros(50)\n","\n","  # Split in K cross-validation folds randomly \n","  K = 5\n","  fold_sze = int(N/K)\n","\n","  # Get the indices\n","  index = [0] * (K+1)\n","  for i in range(1,K+1):\n","    index[i] = int(fold_sze)*(i)\n","\n","  all_folds = [[0] * 1] * (K+1)\n","  all_fold_labels = [[0] * 1] * (K+1)\n","  # Create K folds\n","  start = 1\n","  for k in range(1,K+1):\n","    end = index[k]\n","    fold = s[start-1:end]\n","    fold_label = labels[start-1:end]\n","\n","    all_folds[k] = fold\n","    all_fold_labels[k] = fold_label\n","    start = end + 1\n","\n","    # Display Folds \n","    df = pd.DataFrame(data=fold)\n","\n","  all_folds = np.delete(all_folds,0,axis = 0)\n","  all_fold_labels = np.delete(all_fold_labels,0,axis = 0)\n","\n","  df = pd.DataFrame(data=all_folds)\n","\n","  return all_folds, all_fold_labels"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ka-mHey6kDhn","executionInfo":{"status":"ok","timestamp":1602003023591,"user_tz":240,"elapsed":3596,"user":{"displayName":"Rebecca Gartenberg","photoUrl":"","userId":"14914130225385476872"}},"outputId":"4963737d-bb02-4fd6-f364-88c674bce6c2","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["for n in range(0,50):\n","\n","  # Incorrect cross validation\n","\n","  model_features = SelectKBest(k=100)\n","  best_features = model_features.fit_transform(s,labels)\n","\n","  all_folds, all_fold_labels = split_data(best_features)\n","\n","\n","\n","  correct = 0\n","\n","  for k in range(0,K):\n","    training_data = np.concatenate([all_folds[i] for i in range(5) if (i != k)]) # This line was given to us by Andrew Lorber\n","    training_labels = np.concatenate([all_fold_labels[i] for i in range(5) if(i != k)])\n","    #training_data_subset = np.concatenate([best_features[i] for i in range(5) if (i != k)]) # This line was given to us by Andrew Lorber\n","    #print(training_data_subset)\n","    #training_labels_subset = np.concatenate([all_fold_labels[i] for i in range(5) if (i != k)])\n","    \n","    testing_data = all_folds[k]\n","    testing_labels = all_fold_labels[k]\n","    \n","    # Display training data (all samples but kth fold)\n","    df = pd.DataFrame(data=training_data)\n","\n","    neigh = KNeighborsClassifier(n_neighbors=1)\n","    neigh.fit(training_data, training_labels)\n","\n","    #predicted_labels = neigh.predict(model_features.transform(testing_data))\n","    predicted_labels = neigh.predict((testing_data))\n","\n","    for i in range(0,len(predicted_labels)):\n","      if predicted_labels[i] == testing_labels[i]:\n","        correct = correct + 1\n","  percent_correct_incorrect[n] = correct/(K*len(testing_labels))\n","\n","\n","  # K-fold Cross Validation\n","  correct = 0\n","\n","  for k in range(0,K):\n","    all_folds, all_fold_labels = split_data(s)\n","    training_data = np.concatenate([all_folds[i] for i in range(5) if (i != k)]) # This line was given to us by Andrew Lorber\n","    training_labels = np.concatenate([all_fold_labels[i] for i in range(5) if (i != k)])\n","    testing_data = all_folds[k]\n","    testing_labels = all_fold_labels[k]\n","    # Display training data (all samples but kth fold)\n","    df = pd.DataFrame(data=training_data)\n","\n","    model_features = SelectKBest(k=100)\n","    best_features = model_features.fit_transform(training_data,training_labels)\n","\n","    neigh = KNeighborsClassifier(n_neighbors=1)\n","    neigh.fit(best_features,training_labels)\n","\n","    predicted_labels = neigh.predict(model_features.transform(testing_data))\n","    for i in range(0,len(predicted_labels)):\n","      if predicted_labels[i] == testing_labels[i]:\n","        correct = correct + 1\n","        \n","  percent_correct_correct[n] = correct/(K*len(testing_labels))\n","\n","\n","print('Incorrect Method Percent Correct =', np.mean(percent_correct_incorrect) * 100)\n","print('Correct Method Percent Correct =', np.mean(percent_correct_correct) * 100)\n","\n","\n"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Incorrect Method Percent Correct = 98.0\n","Correct Method Percent Correct = 50.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fbGci6DIUr6P"},"source":["# Model Assessment and Selection\n","\n","In this project cross validation is done incorrectly and then correctly. Using the incorrect method, the 100 best predictors are chosen based on all of the samples. The model then overfits and has a very high percent correct, which came out to 98% in this case. When using the correct method for cross validation, each of k folds was used as the testing data while the other k-1 folds were used as the training data. The 100 best predictors are chosen on each set of training data and then trained on the corresponding testing data. The model predicts 50% correct."]}]}